{
  "hash": "4ca486d8f64dd2f83162f6403c93f03a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Estimators\"\ndate-modified: \"2024-09-28\"\n---\n\n\n\n\n## R Packages Used\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n## Random Sample\n\nWhen collecting a random sample, it is believed that the data being collected comes from a probability distribution denoted as $F(\\boldsymbol \\theta)$, where $\\boldsymbol \\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_p)^{\\mathrm T}$ is a vector or parameters that describe the distribution. It is assumed that the random sample is a collection of random variables, denoted as $X_1, \\cdots, X_n$, that are considered iid (identical and independently distributed[^1]). Using this random sample, one infer the value of the parameters $\\boldsymbol \\theta$ by functions (statistics) on the random sample.\n\n[^1]: This means that the random variables $X_1, \\ldots, X_n$, come from the same distribution and the value for one random variable will not influence the value of a different random variable. See [here](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) for more information.\n\n## Statistical Estimators\n\nA statistical estimator is said to be a function designed to provide a point estimate, or interval estimate, of an unknown parameter in $\\boldsymbol \\theta$. Common statistical estimators can be the mean, $\\bar X = \\frac{1}{n}\\sum^n_{i=1} X_i$, or standard deviation, $S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X_i - \\bar X)^2$. Other estimators can be obtained by applying a procedure such as the [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), [method of moments](https://en.wikipedia.org/wiki/Method_of_moments_(statistics)) or a [Bayes Estimator](https://en.wikipedia.org/wiki/Bayes_estimator).\n\n## Sampling Distributions\n\nA sampling distribution can be thought as the distribution of an estimator (statistic). The reason is because the estimator is a function of random variables; therefore, the estimator itself is also a random variable. This means that the estimator will vary based on what was randomly drawn for the sample. For example, $\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i$ will have a distribution depending on the distribution that generated $X_1, \\ldots, X_n$.\n\n### Normal Distribution Example\n\nAssume that $X_1, \\ldots, X_{25}\\overset{iid}{\\sim}N(8, 3)$, normal distribution with mean 8 and variance 3. Depending on the sample, the value of $\\bar X$ will change due to the randomness being generated. Therefore, a different sample will yield a different value of $\\bar X$. The R code below will demonstrate the potential distribution $\\bar X$ by simulating numerous samples from distribution above and generating the histogram of $\\bar X$.\n\n#### \n\nTo simulate a random sample of 25 that follows a normal distribution, we can use the `rnorm` function. Afterwards, we will compute the mean of the sample.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- rnorm(25, 8, sqrt(3))\nmean(x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 7.622139\n```\n\n\n:::\n:::\n\n\n\n\nNotice that the value is close to 8. If we generate two different samples, notice how all means calculated are different from each other.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx2 <- rnorm(25, 8, sqrt(3))\nmean(x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 8.008738\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx3 <- rnorm(25, 8, sqrt(3))\nmean(x3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 7.919917\n```\n\n\n:::\n:::\n\n\n\n\nNow to visualize see the distribution of $\\bar X$, we will simulate 10,000 samples, compute the mean of each sample, and construct the a histogram of the computed means.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 25 \nx_samples <- replicate(10000, rnorm(25, 8, sqrt(3)))\n# Obtain the mean for all the samples\nx_means <- colMeans(x_samples)\n# Plot a histogram of the sample means\ndata.frame(xbar = x_means) |> \n  ggplot(aes(xbar)) +\n  geom_histogram() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nNotice that the values of $\\bar X$ are bell shaped centered around the value 8. This makes us think that the sampling distribution for $\\bar X$ may follow a normal distribution. In fact, if a random is said to be generated from a normal distribution, then the distribution will also be normally distributed. For this example, the distribution of $\\bar X$ is $N(8, 3/25)$. We can plot the probability density function on the histogram and they will closely align.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 8, sqrt(3/25))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n## Central Limit Theorem\n\nThe central limit theorem is the framework for several of hypothesis tests that are based on probability models.\n\n::: callout-note\n### Central Limit Theorem\n\nIf random variables $X_1, X_2, \\cdots, X_n$ are independent come from the same distribution ($iid$), $E(X_i) = \\mu <\\infty$ (finite), $Var(X_i) = \\sigma^2<\\infty$ (finite), then\n\n$$\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt n} \\overset{\\circ}{\\sim} N(0,1)\n$$\n\nas $n\\rightarrow \\infty$, which implies:\n\n$$\n\\bar X \\overset{\\circ}{\\sim}  N(\\mu, \\sigma^2/n)\n$$\n:::\n\nThe central limit theorem allows us to assume the distribution of $\\bar X$ regardless of the distribution of the sample $X_1, X_2, \\cdots, X_n$. The only condition is that the expected value and variance exist.\n\n### $\\chi^2$ Example\n\nAssume that $X_1, \\ldots, X_{25}\\overset{iid}{\\sim}\\chi^2(4)$, Chi-Square distribution with 4 degrees of freedom. According to the central limit theorem, as $n\\rightarrow \\infty$, the distribution for $\\bar X$ will approximately be normal with a mean of $4$ and variance $8/n$. The following examples show how the distribution begin to follow a normal distribution (red line) as $n$ increases 15, 30, 50, 100, 1000.\n\n#### $n = 15$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 15 \n# Obtain the mean for all the samples\nx_samples <- replicate(10000, rchisq(15, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/15))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n#### $n = 30$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 30\n# Obtain the mean for all the samples\nx_samples <- replicate(10000, rchisq(30, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/30))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n#### $n = 50$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 50\n# Obtain the mean for all the samples\nx_samples <- replicate(10000, rchisq(50, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/50))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n#### $n = 100$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 100\n# Obtain the mean for all the samples\nx_samples <- replicate(10000, rchisq(100, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/100))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n#### $n = 1000$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 1000 \n# Obtain the mean for all the samples\nx_samples <- replicate(10000, rchisq(1000, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/1000))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::",
    "supporting": [
      "estimators_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}